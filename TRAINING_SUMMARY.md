# Training Summary Report

> **Project**: Real-Time Bidirectional Continuous Sign Language Interpretation
> **Dataset**: RWTH-PHOENIX-Weather 2014
> **Training Completed**: December 17, 2025

---

## ğŸ¯ Final Results

| Metric | Value |
|--------|-------|
| **Best WER** | **52.85%** |
| **Best Epoch** | 98 |
| **Total Epochs** | 100 |
| **Training Time** | ~7 hours |
| **Best Model** | `checkpoints/hybrid/best.pth` |

---

## ğŸ“Š Training Progression

### WER Over Time

```
Epoch   1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 91.75%
Epoch  10: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             70.25%
Epoch  20: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   57.55%
Epoch  40: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     55.56%
Epoch  60: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     55.04%
Epoch  80: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      53.76%
Epoch  98: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      52.85% â† Best!
Epoch 100: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      53.52%
```

### Key Metrics at Best Epoch (98)

| Metric | Value |
|--------|-------|
| Train Loss | 0.0006 |
| Dev Loss | 4.91 |
| WER | 52.85% |
| Blank Ratio | 69.0% |
| Learning Rate | 2.18e-05 |

---

## ğŸ—ï¸ Architecture Used

### Hybrid CTC + Attention Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ResNet-18 CNN Backbone                     â”‚
â”‚                  (Pretrained on ImageNet)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Transformer Encoder (6 layers)                  â”‚
â”‚                   8 heads, d_model=512                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                               â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚   CTC Head   â”‚              â”‚   Decoder    â”‚
       â”‚  (weight=0.3)â”‚              â”‚  (weight=0.7)â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Model Statistics

| Component | Details |
|-----------|---------|
| Total Parameters | 45.2M |
| Trainable Parameters | 44.5M |
| Encoder Layers | 6 |
| Decoder Layers | 3 |
| CTC Weight | 0.3 |
| CE Weight | 0.7 |

---

## ğŸ“ˆ Comparison with Benchmarks

### PHOENIX-2014 Leaderboard

| Method | WER | Notes |
|--------|-----|-------|
| **SOTA (CorrNet, 2023)** | ~17.8% | Full resolution, advanced techniques |
| **VAC (2021)** | ~19% | Visual Alignment Constraint |
| **Sign Language Transformers (2020)** | ~24% | Original hybrid CTC+Attention |
| **Our Result** | **52.85%** | Limited frames, batch size 2 |
| Baseline (Pure CTC) | ~30% | Published baseline |

### Why Our WER is Higher

| Factor | Impact | Our Setting | Optimal |
|--------|--------|-------------|---------|
| **Max Frames** | High | 64 | 300+ |
| **Batch Size** | Medium | 2 | 8-16 |
| **Resolution** | Medium | 210Ã—260 | Full |
| **Training Time** | Medium | 7 hrs | Days |
| **Data Augmentation** | High | None | Extensive |

---

## ğŸ”§ Improvements to Achieve <30% WER

### 1. Data Augmentation (High Impact)

```python
# Add to dataset
transforms = [
    RandomHorizontalFlip(p=0.5),
    RandomRotation(degrees=10),
    ColorJitter(brightness=0.2, contrast=0.2),
    RandomCrop(scale=(0.8, 1.0)),
    TemporalJitter(frames=Â±5),
    SpeedPerturbation(rates=[0.9, 1.0, 1.1])
]
```

**Expected improvement**: 5-10% WER reduction

### 2. Visual Alignment Constraint (VAC)

Add auxiliary loss to enforce monotonic alignment:

```python
# VAC Loss
vac_loss = alignment_loss(encoder_features, gloss_boundaries)
total_loss = 0.3 * ctc_loss + 0.6 * ce_loss + 0.1 * vac_loss
```

**Expected improvement**: 3-5% WER reduction

### 3. Self-Distillation

Use the model's own predictions for additional supervision:

```python
# Self-distillation
with torch.no_grad():
    soft_targets = model(frames).softmax(dim=-1)
distill_loss = KL_divergence(predictions, soft_targets)
```

**Expected improvement**: 2-3% WER reduction

### 4. Larger Batch Size + More Frames

```python
# Current
batch_size = 2
max_frames = 64

# Improved (requires more GPU memory)
batch_size = 8
max_frames = 150
```

**Expected improvement**: 5-8% WER reduction

### 5. Pretrained Sign Language Features

Use features from a model pretrained on larger sign language datasets:

- I3D features (pretrained on Kinetics)
- S3D features
- VideoMAE features

**Expected improvement**: 8-12% WER reduction

### 6. Multi-Scale Temporal Modeling

Add temporal convolutions at multiple scales:

```python
temporal_conv = nn.ModuleList([
    nn.Conv1d(d_model, d_model, kernel_size=k)
    for k in [3, 5, 7, 11]
])
```

**Expected improvement**: 2-4% WER reduction

---

## ğŸ“ Lessons Learned

### What Worked

1. âœ… **Hybrid CTC + Attention** - Prevented CTC collapse
2. âœ… **ResNet-18 pretrained backbone** - Good visual features
3. âœ… **Warmup + Cosine LR** - Stable training
4. âœ… **Joint loss (0.3 CTC + 0.7 CE)** - Balanced training
5. âœ… **Gradient clipping (max_norm=5)** - Prevented explosions

### What Didn't Work

1. âŒ **Pure CTC** - Collapsed to blank-only output
2. âŒ **High learning rate** - Destabilized training
3. âŒ **No warmup** - Early collapse

### Key Insights

1. **CTC needs auxiliary supervision** - Pure CTC is fragile
2. **Monitor blank ratio** - Early indicator of collapse
3. **Dev loss can increase while WER improves** - Don't rely solely on loss
4. **Pretrained features are crucial** - ImageNet transfer helps

---

## ğŸ“ Files Summary

| File | Purpose |
|------|---------|
| `checkpoints/hybrid/best.pth` | Best model (WER=52.85%) |
| `checkpoints/hybrid/latest.pth` | Final epoch model |
| `checkpoints/hybrid/config.json` | Model configuration |
| `checkpoints/hybrid/vocab.json` | Vocabulary mapping |
| `training_log.jsonl` | Training metrics history |
| `train_hybrid.py` | Training script |
| `evaluate.py` | Evaluation script |

---

## ğŸš€ Next Steps

### Immediate (This Week)

1. [ ] Evaluate on test set
2. [ ] Generate sample predictions
3. [ ] Create demo video

### Short-term (If Time Permits)

1. [ ] Add data augmentation
2. [ ] Increase max_frames to 128
3. [ ] Try batch size 4

### Future Work (Documented for Paper)

1. [ ] Implement VAC loss
2. [ ] Use I3D pretrained features
3. [ ] Multi-scale temporal modeling
4. [ ] Knowledge distillation

---

## ğŸ“š References

1. **Sign Language Transformers** (CamgÃ¶z et al., 2020) - Hybrid CTC+Attention
2. **VAC** (Zhu et al., 2021) - Visual Alignment Constraint
3. **CorrNet** (2023) - Current SOTA on PHOENIX
4. **RWTH-PHOENIX-Weather 2014** - Dataset paper

---

*Report generated: December 17, 2025*
*Project: Semester III Academic Project*


